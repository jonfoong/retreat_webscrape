---
title: "IPI mini webscraping workshop 2024: A quick primer on webscraping in R"
author: Jonah Foong
date: "22 September 2023"
format: 
  revealjs:
    reference-location: document
    scrollable: true
    embed-resources: true
    theme: moon
    background-interactive: true
editor: 
  markdown: 
    wrap: 72
---

## Overview

1. Webscraping and its applications to research
2. Some basics about web pages
3. Webscraping ethics
4. Scraping HTML and JS hands on
5. Automating our webscraper using containers and the cloud*
6. Next steps

* If we have time

## What is webscraping

- Parsing information from web pages using a script, some level of automation

    - pulling rent prices of all apartments on immoscout
    - email addresses of university staff
- Typically unstructured
- Is different to pulling from an API

Ultimately, pulling information that is **raw and unstructured**, and turning it into usable, structured information!

## Applications to research 

- how do people behave online? Massive amounts of textual data on forums/SM
- analysing parliamentary speech data
- where do news organizations lean - headlines and articles
- voting records at the UNGA
- making data accessible via dashboards

## How are websites organized?

Usually organized in 3 ways:
  1. Pure HTML (static pages)
  2. Javscript content (dynamic pages)
  3. Javascript rendered HTML (static pages rendered dynamically!)

## How are websites organized?

### Some important terminology

*CSS selector:*

    - CSS stands for Cascading Style Sheets, controls the styling (font, color, etc) of a website
    - Each element in a website has an ID tagged to it that allows customized styling of that element in the website - this is the CSS selector
    
*xpath:* 

    - Think of a web page as a folder, and pieces of information within a web page are files or sub folders within that folder
    - If a web page takes a folder structure, then paths must exist to the information within

<!-- ::: {.panel-tabset} -->

<!-- ### HTML -->

<!-- - Information you want can be found in source code -->


<!-- ### Javscript -->

<!-- - Information you want cannot be found in source code -->


<!-- ::: -->

# Let's get scraping!

## Ethics

- Do we have permission from the web host? 
  - Most websites will have a `robots.txt` file in the root directory we can check
- Allowable user agents: who is allowed to scrape?
- Respecting rate limits: how often is one allowed to scrape?
  - Usually in the hundreds to thousands per hour

- [Example one- wikipedia](https://en.wikipedia.org/robots.txt){target="_blank"}
- [Example two - twitter](https://twitter.com/robots.txt){target="_blank"}

## Scraping an HTML page (example 1)

- Let's try scraping Wikipedia since it has a very liberal policy
- We are interested in the list of [highest grossing sci-fi films of all time](https://en.wikipedia.org/wiki/List_of_highest-grossing_science_fiction_films)
- Our workhorse package for this is `rvest`
- We first set our user agent to identify ourselves

## Scraping an HTML page (example 2)

- What if the data we want is not in tabular form?
- Say we want all news headlines on a website
- We need a little more dexterity and will have to specify our desired xpath
- xpath syntax is `//tagname[@attribute='value']`
  - These things are hard to remember; there are online tools for this and also chatgpt

## Scraping Javscript rendered pages

- Recall that JS pages are rendered only when we interact with it in some way
- The website sends a request to the server to retrieve information, this info is then loaded onto the page
- Two kinds of requests, GET (request contained in URL) and POST (request contained in body)
- We will look at two examples

    - real time flight information from [flightradar24](https://www.flightradar24.com/)
    - topics and sitting dates of Singapore parliamentary speeches
- Also useful to check if an API for a use case already exists

## Scraping Javscript rendered pages (key steps)

1. Open up developer tools and go to network tab
2. Go the the `Fetch/XHR` tab underneath it
3. Reload the page and let requests populate
  - These are all the requests that a page sends in the background
4. Identify the one you need, typically in JSON format
5. Extract info!

## Scraping Javascript rendered HTML

1. The page looks static and info is in source code, but cannot be scraped
2. Non-interactive so there is no AJAX request sent
3. We can deal with this using a `headless browser`
4. Headless browsers mimic functionality of a real browser except it works in the background
5. They allow page html to load first

## Headless browsing

- We use the `webdriver` package for this which runs on phantomJS (a headless browser)
- We navigate the website using CSS/xpath selectors, very similar to how we did HTML scraping
- The [APSA website](https://convention2.allacademic.com/one/apsa/apsa23/index.php) is a good example: we want to know which sessions went virtual


## Concluding thoughts

- Websites can be organized in tons of other ways, web structures constantly evolving
- Webscraping is specific to use case
- R has poor libraries but enough for most basic uses
- next steps: automate your scraper ([tutorial](https://jonfoong.github.io/writings/rscrapergcp/))








