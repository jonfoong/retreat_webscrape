---
title: "Headless browser scraping"
format: html
---


```{r}

library(rvest)
library(dplyr)
library(webdriver)
library(ggplot2)
library(stringr)
library(kableExtra)

```

We are interested in scraping information from 2023 APSA panels, specifically which panels were virtual and which were not. 

We can first try scraping with `rvest` and see what happens. 

```{r}

url <- "https://convention2.allacademic.com/one/apsa/apsa23/index.php?cmd=Online+Program+View+Selected+Session+Type+Submissions&selected_session_type_id=11507&program_focus=browse_by_session_type_submissions&PHPSESSID=qmnc9o16lkvr4lule02rcp99r3"

raw_html <- read_html(url)

xpath <- '//*[@id="event_list"]/li[3]'

raw_html %>% 
  html_elements(xpath = xpath)


```

Because the html is only loaded after the page is loaded, we can mimic this browser behaviour by using a headless browser. Essentially we allow the page to load first and then extract the html.

We use phantomjs as our headless browser and "click" our way through to the page we want using xpaths. Along the way we can view our progress by tasking the browser with taking a screenshot for us.

```{r}

#setwd("phantomjs/bin")

pjs <- run_phantomjs()

ses <- Session$new(port = pjs$port)

url <- "https://convention2.allacademic.com/one/apsa/apsa23/index.php"

ses$go(url)

ses$takeScreenshot()

```


```{r}

timezone <- ses$findElement(xpath = '//*[@id="new_timezone"]/option[318]')

timezone$click()

ses$takeScreenshot()

```


```{r}

set_timezone <- ses$findElement(xpath='//*[@id="new_timezone_selected"]')

set_timezone$click()

ses$takeScreenshot()

```

```{r}

click_sidebar <- ses$findElement(xpath = '//*[@id="left_panel_menu_column"]/ul[1]/li[8]/a')

click_sidebar$click()

ses$takeScreenshot()

```

```{r}

click_sidebar <- ses$findElement(xpath = '//*[@id="home"]/div[3]/div[2]/div[1]/div/ul/li[7]/a')

click_sidebar$click()

# this takes awhile so we skip

ses$takeScreenshot()

```

Now that our page is loaded, we want to extract the html that was produced. After that we do the same as we would with any HTML input.

```{r}

src <- ses$getSource()

# end session

ses$delete()

sessions <- src %>% 
  read_html() %>% 
  html_elements(xpath = "//a[@class='ul-li-has-alt-left ui-btn ui-btn-icon-right ui-icon-carat-r']") %>% 
  html_text()

head(sessions)

```

Now we can parse this into a structure we can work with. We want to know which sub units were more likely to hold virtual sessions

```{r}

sessions_df <- 
  data.frame(text = sessions) %>% 
  mutate(title = str_extract(text,
                             "(?<=Virtual \\d{1,2})([A-Z].*?)(?=Sub Unit\\:)|(?<=LACC, )(.*?)(?=Sub Unit\\:)") %>% 
           str_remove("\\d+([A-Z](?=[A-Z]|\\())?") %>% 
           str_remove("Petree Hall [A-Z]"),
         sub_unit = str_extract(text,
                                "(?<=Sub Unit\\: )(.*?)(?=Session Submission)") %>% 
           str_remove("Division \\d{1,2}\\: "),
         virtual = ifelse(grepl("Virtual", text), 1, 0)) %>% 
  select(-text) 

# view data structure

sessions_df %>% 
  head() %>% 
  kable()

```

```{r}

# we extract the top 20 sub units

subunit <- sessions_df %>% 
  group_by(sub_unit) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)) %>% 
  slice(1:20) %>% 
  .$sub_unit

sessions_df %>% 
  filter(sub_unit %in% subunit) %>% 
  group_by(sub_unit) %>% 
  summarise(prop_virtual = mean(virtual), n_sessions= n()) %>% 
  arrange(desc(prop_virtual)) %>% 
  kable(digits = 3)

```

We can also plot distribution

```{r}

sessions_df %>% 
  group_by(sub_unit) %>% 
  summarise(prop_virtual = mean(virtual), 
            n_sessions= n()) %>% 
  ggplot(aes(x = prop_virtual, y = after_stat(count)/sum(after_stat(count)))) +
  #geom_density(outline.type = "full") +
  geom_histogram(bins = 10, position = "dodge") +
  scale_x_continuous(breaks = seq(0, 1, 0.1)) +
  ggtitle("Proportion of sub-unit in virtual sessions") +
  theme_minimal() +
  ylab("Proportion")

```

Now do it yourself! (10 mins)

1. Try extracting all titles and sub units of another panel using the same approach, just extract the data don't need to parse or restructure

    - Note that taking screenshots is slow, you might want to skip that
2. Once done, try with another one

