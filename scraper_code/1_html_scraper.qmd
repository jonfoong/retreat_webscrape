---
title: "Basic HTML scraping"
format: html
---

```{r}

library(rvest)
library(dplyr)
library(kableExtra)

```

First we set our user agent. The user agent tells the server what device you are viewing from, your browser, computer OS, etc. We want to specify this to simulate the behavior of a typical web user.

```{r}

ua <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"

httr::set_config(httr::user_agent(ua))

# we can check that our user agent has indeed been set

se <- session("https://example.com")

se$response$request$options$useragent

```

Now let's try scraping a table from wikipedia using a CSS selector

```{r}

# grab url

url <- "https://en.wikipedia.org/wiki/List_of_highest-grossing_science_fiction_films"

# find css/xpath

selector <- ""

# first read HTML

raw_html <- url %>% 
  read_html() 

# now extract the element

tbl_html <- raw_html %>% 
  html_element(selector) 

# we can convert this into a table
tbl <- tbl_html %>% 
  html_table()

tbl %>% head(10) %>% 
  kable()

```

We can also use an xpath instead of css selector. If a web page is a document, an xpath is akin to a file directory and specifies which part of the "document" to look into.

```{r}

xpath <- ''

raw_html %>% 
  html_element(xpath = xpath) %>% 
  html_table() %>% 
  head(10) %>% 
  kable()

```

Let's try scraping headlines from a news website. Here we use xpath because it is more flexible and allows us to select all elements of a certain class.

The xpath syntax is `//tagname[@attribute='value']`

```{r}

# fox news

url <- "https://www.foxnews.com/"

raw_html <- url %>% 
  read_html()

raw_html %>% 
  html_elements(xpath = "") %>% 
  html_text()

# bbc

url <- "https://www.bbc.com/news"

raw_html <- url %>% 
  read_html()

raw_html %>% 
  html_elements(xpath = "") %>% 
  html_text()


```
